A Research-driven project that integrates computer vision, natural language understanding, and robotics within dynamically generated virtual environments. The system enables a robotic assistant to perceive, interpret, and act upon complex natural language instructions in simulated 3D indoor ecosystems that adapt and evolve using generative AI.

Project Overview

This project focuses on building a robust, simulated robotic assistant capable of:
	•	Vision: Understanding and segmenting complex 3D scenes using state-of-the-art vision models.
	•	Language: Processing and comprehending human-level natural language commands using large language models (LLMs).
	•	Robotics: Planning and executing multi-step tasks within virtual indoor environments.
	•	Adaptive Ecosystem: Leveraging generative AI to dynamically create and modify indoor environments, allowing the robot to adapt to new and unforeseen scenarios.
	•	Reinforcement Learning: Utilizing RL to optimize task planning and robot actions in the virtual ecosystem.

Features
	•	Dynamic scene generation with generative AI models
	•	Semantic segmentation and object detection for environment perception
	•	Natural language instruction parsing and understanding
	•	Modular robotic task execution framework
	•	Reinforcement learning for adaptive decision-making

Technologies Used
	•	Unity 3D for simulation environment creation
	•	OpenAI GPT-based Large Language Models for instruction understanding
	•	DETR (Detection Transformer) and SAM (Segment Anything Model) for vision tasks
	•	Reinforcement Learning libraries (e.g., Stable Baselines3)
	•	Python and C# integration for robotics control and simulation
